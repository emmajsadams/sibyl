# EVAL.md â€” Evaluation System Design

## Overview

Three-layer eval system that measures game balance, decision quality, and enables human feedback. Feeds back into SFT training data quality filtering.

## Layer 1: Automated Metrics (from game JSON)

Extract per-game and aggregate metrics from `training/*.json`:

### Game-Level Metrics
- **Game length** â€” rounds played (healthy: 3â€“15; <3 = steamroll, >15 = stalemate)
- **Winner** â€” which side won
- **Surviving units** â€” count and HP remaining
- **Total damage dealt** â€” per side

### Class-Level Metrics
- **Win rate by class** â€” flag if any class consistently over/underperforms across N games
- **Damage per game** â€” average damage dealt by each class
- **Survival rate** â€” how often each class survives to end
- **Ability usage distribution** â€” are all abilities being used? Unused = bad prompts or bad design

### Decision-Level Metrics
- **Action success rate** â€” how often units attempt invalid actions (occupied tile, out of range, etc.). Measures prompt quality
- **Actions per turn** â€” are units using both action slots or wasting them?
- **Breach impact score** â€” damage dealt by breached units to own team vs cost to set up

### Output
- `eval/metrics/{gameId}.json` â€” per-game metrics
- `eval/aggregate.json` â€” rolling aggregate across all games
- `bun run eval` â€” run metrics on all games
- `bun run eval:report` â€” markdown summary

---

## Layer 2: LLM-as-Judge (automated quality scoring)

Feed game logs to a judge model that scores decision quality:

### Per-Decision Scoring (1â€“5)
- **Tactical quality** â€” was this a good move given the board state?
- **Prompt adherence** â€” is the unit following its class prompt?
- **Reasoning quality** â€” does the `ğŸ’­` thinking line show sound logic?
- **Outcome alignment** â€” did the action achieve what the reasoning predicted?

### Flags
- **Confused turns** â€” reasoning contradicts the action taken
- **Wasted turns** â€” both actions are `wait` or fail
- **Friendly fire** â€” attacking own team (unless breached)
- **Suicidal positioning** â€” moving adjacent to lethal threats at low HP

### Output
- `eval/judge/{gameId}.json` â€” per-decision scores
- Judge model: use a different/stronger model than the game agent to avoid self-evaluation bias

---

## Layer 3: Human-in-Loop

### Post-Game Summary Card
After each game, post to #sibyl with:
- Win/loss, turns, surviving units
- Automated metrics highlights (any flags?)
- Judge scores summary (worst-rated decisions)
- React with ğŸ‘/ğŸ‘ for overall game quality

### Decision Review
- On ğŸ‘, trigger deeper analysis of flagged decisions
- Human can annotate specific turns with notes
- Stored as `eval/feedback/{gameId}.jsonl`:
  ```jsonl
  {"turnId": "t3-p-Hawk", "rating": 1, "note": "should have retreated instead of engaging 1v3"}
  {"gameId": "v0.5.13-88", "rating": 4, "note": "good game, breach timing was smart"}
  ```

---

## SFT Quality Filtering

Use eval data to improve training:

1. **Filter by game quality** â€” only train on games rated above threshold
2. **Filter by decision quality** â€” exclude turns with judge score <3
3. **Weight by quality** â€” higher-scored examples get more weight
4. **Track model provenance** â€” compare eval scores across models to measure improvement

### Model Tracking
Every training example includes the model that generated it:
- `metadata.model` in SFT JSONL (e.g., `"claude-sonnet-4-20250514"`)
- `model` field in training JSON game records
- Enables A/B comparison: does fine-tuned model score better than base?

---

## Implementation Order

1. **Model tracking** â€” add to training data + backport existing runs âœ…
2. **Layer 1** â€” `src/eval/metrics.ts`, `bun run eval`
3. **Layer 2** â€” `src/eval/judge.ts`, `bun run eval:judge`
4. **Layer 3** â€” post-game summary in balance cycle, feedback storage
5. **SFT integration** â€” quality filtering in `sft/src/convert.ts`
